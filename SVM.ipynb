{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "dataset= pd.read_csv('dataset.csv')\n",
    "print(\"Dataset Size:\", dataset.shape)\n",
    "\n",
    "# Replace 'Unknown' with NaN in Target\n",
    "dataset['Genetic Disorder'].replace('Unknown', np.nan, inplace=True)\n",
    "dataset['Disorder Subclass'].replace('Unknown', np.nan, inplace=True)\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "# Alocate features and labels\n",
    "\n",
    "X = dataset.iloc[:, :-2]  # Features\n",
    "y = dataset.iloc[:, -2:]  # Labels (last two columns)\n",
    "\n",
    "# Perform one-hot encoding on the categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "# drop_first is used to drop one of the columns for each categorical feature to avoid multicollinearity.\n",
    "\n",
    "# Check if the target variable has more than 2 classes\n",
    "if y.nunique().any() > 2:\n",
    "    y = pd.get_dummies(y, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without 'unknown' class removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation, validation and test\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, train_size=0.5,  random_state=1) # training as 50%\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1) # test as 25% and validations as 25%\n",
    "\n",
    "# Select and train model of classifier\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=1)\n",
    "\n",
    "clf = MultiOutputClassifier(svm_classifier)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_validation)\n",
    "\n",
    "y_validation_ds = y_validation.iloc[:, -1]\n",
    "y_validation_gd = y_validation.iloc[:, -2]\n",
    "\n",
    "y_pred_ds = y_pred[:, -1]\n",
    "y_pred_gd = y_pred[:, -2]\n",
    "\n",
    "# Apply metrics\n",
    "\n",
    "accuracy_ds = accuracy_score(y_validation_ds, y_pred_ds)\n",
    "precision_ds = precision_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "recall_ds = recall_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "f1_ds = f1_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "\n",
    "accuracy_gd = accuracy_score(y_validation_gd, y_pred_gd)\n",
    "precision_gd= precision_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "recall_gd = recall_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "f1_gd = f1_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Metrics for the Generative Disorder:\")\n",
    "print(\"Accuracy:\", accuracy_gd)\n",
    "print(\"Precision:\", precision_gd)\n",
    "print(\"Recall:\", recall_gd)\n",
    "print(\"F1 Score:\", f1_gd)\n",
    "\n",
    "print(\"\\nMetrics for the Disorder Subclass:\")\n",
    "print(\"Accuracy:\", accuracy_ds)\n",
    "print(\"Precision:\", precision_ds)\n",
    "print(\"Recall:\", recall_ds)\n",
    "print(\"F1 Score:\", f1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 'unknown' class removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation, validation and test\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, train_size=0.5,  random_state=1) # training as 50%\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1) # test as 25% and validations as 25%\n",
    "\n",
    "# Select and train model of classifier\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=1, class_weight='balanced')\n",
    "\n",
    "clf = MultiOutputClassifier(svm_classifier)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_validation)\n",
    "\n",
    "y_validation_ds = y_validation.iloc[:, -1]\n",
    "y_validation_gd = y_validation.iloc[:, -2]\n",
    "\n",
    "y_pred_ds = y_pred[:, -1]\n",
    "y_pred_gd = y_pred[:, -2]\n",
    "\n",
    "# Apply metrics\n",
    "\n",
    "accuracy_ds = accuracy_score(y_validation_ds, y_pred_ds)\n",
    "precision_ds = precision_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "recall_ds = recall_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "f1_ds = f1_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "\n",
    "accuracy_gd = accuracy_score(y_validation_gd, y_pred_gd)\n",
    "precision_gd= precision_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "recall_gd = recall_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "f1_gd = f1_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Metrics for the Generative Disorder:\")\n",
    "print(\"Accuracy:\", accuracy_gd)\n",
    "print(\"Precision:\", precision_gd)\n",
    "print(\"Recall:\", recall_gd)\n",
    "print(\"F1 Score:\", f1_gd)\n",
    "\n",
    "print(\"\\nMetrics for the Disorder Subclass:\")\n",
    "print(\"Accuracy:\", accuracy_ds)\n",
    "print(\"Precision:\", precision_ds)\n",
    "print(\"Recall:\", recall_ds)\n",
    "print(\"F1 Score:\", f1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Disorder\n",
    "\n",
    "\n",
    "class_labels_last_column = y.iloc[:, -2]\n",
    "\n",
    "# Count the number of occurrences of each class\n",
    "class_counts_gd = class_labels_last_column.value_counts()\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "total_samples = len(class_labels_last_column)\n",
    "class_percentages = (class_counts_gd / total_samples) * 100\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts_gd.index, class_counts_gd, color='pink', alpha=0.7)\n",
    "\n",
    "# Adding percentage text on top of each bar\n",
    "for label, count, percentage in zip(class_counts_gd.index, class_counts_gd, class_percentages):\n",
    "    plt.text(label, count, f'{percentage:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Class Distribution - Generative Disorder')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disorder Subclass\n",
    "\n",
    "# Assuming y is your original dataset\n",
    "class_labels_last_column = y.iloc[:, -1]\n",
    "\n",
    "# Count the number of occurrences of each class\n",
    "class_counts_ds = class_labels_last_column.value_counts()\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "total_samples = len(class_labels_last_column)\n",
    "class_percentages = (class_counts_ds / total_samples) * 100\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts_ds.index, class_counts_ds, color='blue', alpha=0.7)\n",
    "\n",
    "# Adding percentage text on top of each bar\n",
    "for label, count, percentage in zip(class_counts_ds.index, class_counts_ds, class_percentages):\n",
    "    plt.text(label, count, f'{percentage:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Class Distribution - Disorder Subclass')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classes balanced by the classifier parameter (automatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation, validation and test\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, train_size=0.5,  random_state=1) # training as 50%\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1) # test as 25% and validations as 25%\n",
    "\n",
    "# Select and train model of classifier\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=1, class_weight='balanced')\n",
    "\n",
    "clf = MultiOutputClassifier(svm_classifier)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_validation)\n",
    "\n",
    "y_validation_ds = y_validation.iloc[:, -1]\n",
    "y_validation_gd = y_validation.iloc[:, -2]\n",
    "\n",
    "y_pred_ds = y_pred[:, -1]\n",
    "y_pred_gd = y_pred[:, -2]\n",
    "\n",
    "# Apply metrics\n",
    "\n",
    "accuracy_ds = accuracy_score(y_validation_ds, y_pred_ds)\n",
    "precision_ds = precision_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "recall_ds = recall_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "f1_ds = f1_score(y_validation_ds, y_pred_ds, average='weighted')\n",
    "\n",
    "accuracy_gd = accuracy_score(y_validation_gd, y_pred_gd)\n",
    "precision_gd= precision_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "recall_gd = recall_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "f1_gd = f1_score(y_validation_gd, y_pred_gd, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Metrics for the Generative Disorder:\")\n",
    "print(\"Accuracy:\", accuracy_gd)\n",
    "print(\"Precision:\", precision_gd)\n",
    "print(\"Recall:\", recall_gd)\n",
    "print(\"F1 Score:\", f1_gd)\n",
    "\n",
    "print(\"\\nMetrics for the Disorder Subclass:\")\n",
    "print(\"Accuracy:\", accuracy_ds)\n",
    "print(\"Precision:\", precision_ds)\n",
    "print(\"Recall:\", recall_ds)\n",
    "print(\"F1 Score:\", f1_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
