{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, hamming_loss, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "provided_weights_genetic_disorder = {\n",
    "    'Mitochondrial genetic inheritance disorders': 0.48794813542417026,\n",
    "    'Single-gene inheritance diseases': 0.6160580705934504,\n",
    "    'Multifactorial genetic inheritance disorders': 0.8959937939823793\n",
    "}\n",
    "\n",
    "provided_weights_subclass_disorder = {\n",
    "    'Leigh syndrome': 0.740510888236272,\n",
    "    'Mitochondrial myopathy': 0.7799634288247355,\n",
    "    'Cystic fibrosis': 0.8257328087770821,\n",
    "    'Tay-Sachs': 0.8583698121571453,\n",
    "    'Diabetes': 0.9084058292236937,\n",
    "    'Hemochromatosis': 0.9319554496592232,\n",
    "    \"Leber's hereditary optic neuropathy\": 0.9674738183631628,\n",
    "    \"Alzheimer's\": 0.9926303540754696,\n",
    "    'Cancer': 0.9949576106832161\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset= pd.read_csv('dataset.csv')\n",
    "\n",
    "# Replace 'Unknown' with NaN\n",
    "dataset.replace('Unknown', np.nan, inplace=True)\n",
    "\n",
    "# Eliminate samples with\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "categorical_columns = [\n",
    "    'Genes in mother\\'s side', 'Inherited from father', 'Maternal gene', 'Paternal gene', 'Status',\n",
    "    'Respiratory Rate (breaths/min)', 'Heart Rate (rates/min)', 'Follow-up',\n",
    "    'Gender', 'Birth asphyxia', 'Autopsy shows birth defect (if applicable)', 'Place of birth',\n",
    "    'Folic acid details (peri-conceptional)',\n",
    "    'H/O serious maternal illness', 'H/O radiation exposure (x-ray)', 'H/O substance abuse', \n",
    "    'Assisted conception IVF/ART', 'History of anomalies in previous pregnancies',\n",
    "    'Birth defects', 'Blood test result',\n",
    "    'Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5'\n",
    "]\n",
    "\n",
    "quantitative_with_unknowns_or_ordered_columns = [\n",
    "    'Patient Age', \"Mother's age\", \"Father's age\", 'No. of previous abortion',\n",
    "    'White Blood cell count (thousand per microliter)']\n",
    "\n",
    "\n",
    "for column in categorical_columns:\n",
    "    dataset[column] = label_encoder.fit_transform(dataset[column].astype(str))\n",
    "\n",
    "# Create a copy for encoding\n",
    "dataset_encoded = dataset.copy()\n",
    "\n",
    "# Encode target variables 'Genetic Disorder' and 'Disorder Subclass'\n",
    "# Create mapping\n",
    "genetic_disorder_mapping = {label: i for i, label in enumerate(dataset_encoded['Genetic Disorder'].unique()) if pd.notna(label)}\n",
    "disorder_subclass_mapping = {label: i for i, label in enumerate(dataset_encoded['Disorder Subclass'].unique())}\n",
    "\n",
    "# Replace each original value with its corresponding encoded value as per the mapping\n",
    "dataset_encoded['Genetic Disorder'] = dataset_encoded['Genetic Disorder'].map(genetic_disorder_mapping)\n",
    "dataset_encoded['Disorder Subclass'] = dataset_encoded['Disorder Subclass'].map(disorder_subclass_mapping)\n",
    "\n",
    "\n",
    "# Correspond weights to classes\n",
    "# Get values mapped\n",
    "encoded_values_genetic_disorder = dataset_encoded['Genetic Disorder'].unique()\n",
    "encoded_values_disorder_subclass = dataset_encoded['Disorder Subclass'].unique()\n",
    "\n",
    "# Inverse mappings to get back the original names\n",
    "inverse_genetic_disorder_mapping = {i: label for label, i in genetic_disorder_mapping.items()}\n",
    "inverse_disorder_subclass_mapping = {i: label for label, i in disorder_subclass_mapping.items()}\n",
    "\n",
    "# Map encoded values back to original names\n",
    "names_genetic_disorder = [inverse_genetic_disorder_mapping[i] for i in encoded_values_genetic_disorder]\n",
    "names_disorder_subclass = [inverse_disorder_subclass_mapping[i] for i in encoded_values_disorder_subclass]\n",
    "\n",
    "# Associate weights with encoded values\n",
    "weights_genetic_disorder = {encoded_value: provided_weights_genetic_disorder[name] for encoded_value, name in zip(encoded_values_genetic_disorder, names_genetic_disorder)}\n",
    "weights_disorder_subclass = {encoded_value: provided_weights_subclass_disorder[name] for encoded_value, name in zip(encoded_values_disorder_subclass, names_disorder_subclass)}\n",
    "\n",
    "# Combine both class weights into a dictionary\n",
    "class_weights = {'Genetic Disorder': weights_genetic_disorder, 'Disorder Subclass': weights_disorder_subclass}\n",
    "\n",
    "\n",
    "# Allocate features and labels\n",
    "X = dataset_encoded.drop(columns=['Genetic Disorder', 'Disorder Subclass'])\n",
    "y = dataset_encoded[['Genetic Disorder', 'Disorder Subclass']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and train model of classifier\n",
    "\n",
    "# Best Hyperparameters: {'estimator__criterion': 'gini', 'estimator__max_depth': 10, 'estimator__max_features': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 10, 'estimator__splitter': 'random'}\n",
    "# Best Mean Accuracy: 0.2690801625415589\n",
    "\n",
    "'''\n",
    "dt = MultiOutputClassifier(tree.DecisionTreeClassifier(criterion= 'gini', max_depth= 10, max_features= None, min_samples_leaf= 1, min_samples_split= 10, splitter = 'random', random_state = 1, class_weight = class_weights)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "cv_scores = cross_val_score(dt, X_train, y_train, cv=kf)\n",
    "\n",
    "print(\"Cross-validated scores:\", cv_scores)\n",
    "print(\"Mean cross-validated score:\", cv_scores.mean())\n",
    "\n",
    "'''\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n",
    "\n",
    "\n",
    "# Define the class weights for both outputs\n",
    "class_weights_genetic_disorder = {0: 0.48794813542417026, 1: 0.8959937939823793, 2: 0.6160580705934504}\n",
    "class_weights_disorder_subclass = {0: 0.740510888236272, 1: 0.9084058292236937, 2: 0.7799634288247355, 3: 0.8257328087770821, 4: 0.8583698121571453, 5: 0.9674738183631628, 6: 0.9319554496592232, 7: 0.9926303540754696, 8: 0.9949576106832161}\n",
    "\n",
    "# DecisionTreeClassifier with specified parameters and class weights\n",
    "dt_classifier_genetic_disorder = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1,\n",
    "    class_weight=class_weights_genetic_disorder\n",
    ")\n",
    "\n",
    "dt_classifier_disorder_subclass = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1,\n",
    "    class_weight=class_weights_disorder_subclass\n",
    ")\n",
    "\n",
    "# Create a MultiOutputClassifier\n",
    "dt = MultiOutputClassifier(estimator=DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1\n",
    "), n_jobs=-1)\n",
    "'''\n",
    "# Create a decision tree classifier with sample weights\n",
    "dt = MultiOutputClassifier(\n",
    "    DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=10,\n",
    "        max_features=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=10,\n",
    "        splitter='random',\n",
    "        random_state=1,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    ")'''\n",
    "\n",
    "# Fit the model with sample weights\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred = dt.predict(X_test)\n",
    "proba_pred = dt.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 12 targets to evaluate model\n",
    "'''y_test_columns = y_test.columns[-12:] \n",
    "y_pred_columns = y_pred[:, -12:]\n",
    "\n",
    "balanced_acc_scores_ = {}\n",
    "\n",
    "for i, column in enumerate(y_test_columns):\n",
    "    y_test_ = y_test[column]\n",
    "    y_pred_ = y_pred_columns[:, i]\n",
    "    \n",
    "    balanced_acc_scores_[column] = balanced_accuracy_score(y_test_, y_pred_)'''\n",
    "# y_test_new is DataFrames\n",
    "y_test_columns = y_test.columns[:]\n",
    "# Convert y_pred_new to a DataFrame\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test_columns)\n",
    "\n",
    "balanced_acc_scores = {}\n",
    "\n",
    "for column in y_test_columns:\n",
    "    y_test_column = y_test[column]\n",
    "    y_pred_column = y_pred_df[column]\n",
    "    \n",
    "    balanced_acc_scores[column] = balanced_accuracy_score(y_test_column, y_pred_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for Genetic Disorder:\n",
      "Hamming Loss for Genetic Disorder: 0.5099601593625498\n",
      "F1 Score for Genetic Disorder: 0.48561448108188154\n",
      "Recall for Genetic Disorder: 0.4900398406374502\n",
      "Balanced Accuracy for Genetic Disorder: 0.4161086335206483\n",
      "\n",
      "Classification for Disorder Subclass:\n",
      "Hamming Loss for Disorder Subclass: 0.6972111553784861\n",
      "F1 Score for Disorder Subclass: 0.3009299784990234\n",
      "Recall for Disorder Subclass: 0.30278884462151395\n",
      "Balanced Accuracy for Disorder Subclass: 0.21088086749851453\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for each output separately\n",
    "\n",
    "y_test_array = y_test.to_numpy()\n",
    "\n",
    "hamming_loss_genetic_disorder = hamming_loss(y_test_array[:, 0], y_pred[:, 0])\n",
    "hamming_loss_disorder_subclass = hamming_loss(y_test_array[:, 1], y_pred[:, 1])\n",
    "\n",
    "f1_score_genetic_disorder = f1_score(y_test_array[:, 0], y_pred[:, 0], average='weighted')\n",
    "f1_score_disorder_subclass = f1_score(y_test_array[:, 1], y_pred[:, 1], average='weighted')\n",
    "\n",
    "recall_genetic_disorder = recall_score(y_test_array[:, 0], y_pred[:, 0], average='weighted')\n",
    "recall_disorder_subclass = recall_score(y_test_array[:, 1], y_pred[:, 1], average='weighted')\n",
    "\n",
    "balanced_accuracy_genetic_disorder = balanced_accuracy_score(y_test_array[:, 0], y_pred[:, 0])\n",
    "balanced_accuracy_disorder_subclass = balanced_accuracy_score(y_test_array[:, 1], y_pred[:, 1])\n",
    "\n",
    "# Calculate Hamming loss for each output separately\n",
    "\n",
    "print(\"Classification for Genetic Disorder:\")\n",
    "\n",
    "print(\"Hamming Loss for Genetic Disorder:\", hamming_loss_genetic_disorder)\n",
    "print(\"F1 Score for Genetic Disorder:\", f1_score_genetic_disorder)\n",
    "print(\"Recall for Genetic Disorder:\", recall_genetic_disorder)\n",
    "print(\"Balanced Accuracy for Genetic Disorder:\", balanced_accuracy_genetic_disorder)\n",
    "\n",
    "print(\"\\nClassification for Disorder Subclass:\")\n",
    "\n",
    "print(\"Hamming Loss for Disorder Subclass:\", hamming_loss_disorder_subclass)\n",
    "print(\"F1 Score for Disorder Subclass:\", f1_score_disorder_subclass)\n",
    "print(\"Recall for Disorder Subclass:\", recall_disorder_subclass)\n",
    "print(\"Balanced Accuracy for Disorder Subclass:\", balanced_accuracy_disorder_subclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dataset without Cancer and Alzeimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named dataset\n",
    "filtered_dataset = dataset[(dataset['Disorder Subclass'] != \"Cancer\") & (dataset['Disorder Subclass'] != \"Alzheimer's\")]\n",
    "\n",
    "# Update your X and y with the filtered dataset\n",
    "X_filtered = filtered_dataset.drop(columns=['Genetic Disorder', 'Disorder Subclass'])\n",
    "y_filtered = filtered_dataset[['Genetic Disorder', 'Disorder Subclass']]\n",
    "\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_filtered, y_filtered, train_size=0.75,  random_state=1) # training as 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and train model of classifier\n",
    "\n",
    "# Best Hyperparameters: {'estimator__criterion': 'gini', 'estimator__max_depth': 10, 'estimator__max_features': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 10, 'estimator__splitter': 'random'}\n",
    "# Best Mean Accuracy: 0.2690801625415589\n",
    "'''\n",
    "dt = MultiOutputClassifier(tree.DecisionTreeClassifier(criterion= 'gini', max_depth= 10, max_features= None, min_samples_leaf= 1, min_samples_split= 10, splitter = 'random', random_state = 1))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "cv_scores = cross_val_score(dt, X_train, y_train, cv=kf)\n",
    "\n",
    "print(\"Cross-validated scores:\", cv_scores)\n",
    "print(\"Mean cross-validated score:\", cv_scores.mean())'''\n",
    "\n",
    "\n",
    "# Define the class weights for both outputs\n",
    "class_weights_genetic_disorder = {0: 0.48794813542417026, 1: 0.8959937939823793, 2: 0.6160580705934504}\n",
    "class_weights_disorder_subclass = {0: 0.740510888236272, 1: 0.9084058292236937, 2: 0.7799634288247355, 3: 0.8257328087770821, 4: 0.8583698121571453, 5: 0.9674738183631628, 6: 0.9319554496592232, 7: 0.9926303540754696, 8: 0.9949576106832161}\n",
    "\n",
    "# DecisionTreeClassifier with specified parameters and class weights\n",
    "dt_classifier_genetic_disorder_new = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1,\n",
    "    class_weight=class_weights_genetic_disorder\n",
    ")\n",
    "\n",
    "dt_classifier_disorder_subclass_new = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1,\n",
    "    class_weight=class_weights_disorder_subclass\n",
    ")\n",
    "\n",
    "# Create a MultiOutputClassifier\n",
    "dt_new = MultiOutputClassifier(estimator=DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    splitter='random',\n",
    "    random_state=1\n",
    "), n_jobs=-1)\n",
    "\n",
    "# Fit the model with sample weights\n",
    "dt_new.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred_new = dt_new.predict(X_test_new)\n",
    "proba_pred_new = dt_new.predict_proba(X_test_new) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 12 targets to evaluate model\n",
    "'''y_test_columns = y_test.columns[-12:] \n",
    "y_pred_columns = y_pred[:, -12:]\n",
    "\n",
    "balanced_acc_scores_ = {}\n",
    "\n",
    "for i, column in enumerate(y_test_columns):\n",
    "    y_test_ = y_test[column]\n",
    "    y_pred_ = y_pred_columns[:, i]\n",
    "    \n",
    "    balanced_acc_scores_[column] = balanced_accuracy_score(y_test_, y_pred_)'''\n",
    "# y_test_new is DataFrames\n",
    "y_test_columns_new = y_test_new.columns[-2:]\n",
    "# Convert y_pred_new to a DataFrame\n",
    "y_pred_df_new = pd.DataFrame(y_pred_new, columns=y_test_columns_new)\n",
    "\n",
    "balanced_acc_scores_new = {}\n",
    "\n",
    "for column in y_test_columns_new:\n",
    "    y_test_column_new = y_test_new[column]\n",
    "    y_pred_column_new = y_pred_df_new[column]\n",
    "    \n",
    "    balanced_acc_scores_new[column] = balanced_accuracy_score(y_test_column_new, y_pred_column_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for Genetic Disorder:\n",
      "Hamming Loss for Genetic Disorder: 0.5220883534136547\n",
      "F1 Score for Genetic Disorder: 0.4719688442164553\n",
      "Recall for Genetic Disorder: 0.4779116465863454\n",
      "Balanced Accuracy for Genetic Disorder: 0.41021505376344086\n",
      "\n",
      "Classification for Disorder Subclass:\n",
      "Hamming Loss for Disorder Subclass: 0.6867469879518072\n",
      "F1 Score for Disorder Subclass: 0.29957827384538377\n",
      "Recall for Disorder Subclass: 0.3132530120481928\n",
      "Balanced Accuracy for Disorder Subclass: 0.2795609057019937\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report for each output separately\n",
    "\n",
    "y_test_array_new = y_test_new.to_numpy()\n",
    "\n",
    "hamming_loss_genetic_disorder = hamming_loss(y_test_array_new[:, 0], y_pred_new[:, 0])\n",
    "hamming_loss_disorder_subclass = hamming_loss(y_test_array_new[:, 1], y_pred_new[:, 1])\n",
    "\n",
    "f1_score_genetic_disorder = f1_score(y_test_array_new[:, 0], y_pred_new[:, 0], average='weighted')\n",
    "f1_score_disorder_subclass = f1_score(y_test_array_new[:, 1], y_pred_new[:, 1], average='weighted')\n",
    "\n",
    "recall_genetic_disorder = recall_score(y_test_array_new[:, 0], y_pred_new[:, 0], average='weighted')\n",
    "recall_disorder_subclass = recall_score(y_test_array_new[:, 1], y_pred_new[:, 1], average='weighted')\n",
    "\n",
    "balanced_accuracy_genetic_disorder = balanced_accuracy_score(y_test_array_new[:, 0], y_pred_new[:, 0])\n",
    "balanced_accuracy_disorder_subclass = balanced_accuracy_score(y_test_array_new[:, 1], y_pred_new[:, 1])\n",
    "\n",
    "# Calculate Hamming loss for each output separately\n",
    "\n",
    "print(\"Classification for Genetic Disorder:\")\n",
    "\n",
    "print(\"Hamming Loss for Genetic Disorder:\", hamming_loss_genetic_disorder)\n",
    "print(\"F1 Score for Genetic Disorder:\", f1_score_genetic_disorder)\n",
    "print(\"Recall for Genetic Disorder:\", recall_genetic_disorder)\n",
    "print(\"Balanced Accuracy for Genetic Disorder:\", balanced_accuracy_genetic_disorder)\n",
    "\n",
    "print(\"\\nClassification for Disorder Subclass:\")\n",
    "\n",
    "print(\"Hamming Loss for Disorder Subclass:\", hamming_loss_disorder_subclass)\n",
    "print(\"F1 Score for Disorder Subclass:\", f1_score_disorder_subclass)\n",
    "print(\"Recall for Disorder Subclass:\", recall_disorder_subclass)\n",
    "print(\"Balanced Accuracy for Disorder Subclass:\", balanced_accuracy_disorder_subclass)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ParameterGrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m dt \u001b[38;5;241m=\u001b[39m MultiOutputClassifier(tree\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate all combinations\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m grid \u001b[38;5;241m=\u001b[39m ParameterGrid(param_grid)\n\u001b[0;32m     16\u001b[0m all_mean_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m grid:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ParameterGrid' is not defined"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'estimator__criterion': ['gini', 'entropy'],\n",
    "    'estimator__splitter': ['best', 'random'],\n",
    "    'estimator__max_depth': [None, 10, 20, 30],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],\n",
    "    'estimator__max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dt = MultiOutputClassifier(tree.DecisionTreeClassifier())\n",
    "\n",
    "# Generate all combinations\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "\n",
    "all_mean_scores = []\n",
    "for params in grid:\n",
    "    dt.set_params(**params)\n",
    "\n",
    "    scores = cross_val_score(dt, X_train, y_train, cv=5)\n",
    "  \n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "    all_mean_scores.append((params, mean_accuracy))\n",
    "    \n",
    "    print(f'Parameters: {params}, Mean Accuracy: {mean_accuracy}')\n",
    "\n",
    "# Find the best hyperparameters based on the highest mean accuracy\n",
    "best_params, best_mean_accuracy = max(all_mean_scores, key=lambda x: x[1])\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Mean Accuracy:\", best_mean_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
